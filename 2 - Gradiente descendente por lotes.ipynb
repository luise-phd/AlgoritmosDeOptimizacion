{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNBWdl+NO62quKr41dqlBUK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Problema\n","Ajustar una lÃ­nea recta $ğ‘¦=ğ‘¤ğ‘¥+ğ‘$ a un conjunto de datos, minimizando el error cuadrÃ¡tico medio (MSE) entre las predicciones y los valores reales. En otras palabras, encontrar una funciÃ³n lineal que relacione una variable independiente ğ‘¥ con una variable dependiente ğ‘¦.\n","\n","Donde:\n","\n","* ğ‘¦ â†’ es la variable objetivo (lo que queremos predecir).\n","* ğ‘¥ â†’ es la variable de entrada o predictor.\n","* ğ‘¤ â†’ es el peso o pendiente de la recta (cuÃ¡nto cambia ğ‘¦ por cada unidad que cambia ğ‘¥).\n","* ğ‘ â†’ es el sesgo o intersecciÃ³n con el eje Y (el valor de ğ‘¦ cuando $ğ‘¥=0$).\n","\n","El objetivo es encontrar los valores Ã³ptimos de ğ‘¤ y ğ‘ que permitan que esta recta se aproxime lo mejor posible a los datos reales. Es decir, para cada punto de datos (ğ‘¥ğ‘–, ğ‘¦ğ‘–), queremos que la predicciÃ³n:\n","\n","$$\n","\\hat{y}_i = w x_i + b\n","$$\n","\n","sea lo mÃ¡s cercana posible a la verdadera salida ğ‘¦ğ‘–.\n","\n","Este ejemplo es una forma bÃ¡sica de inteligencia artificial porque implica que un modelo aprende automÃ¡ticamente una relaciÃ³n entre variables a partir de datos, usando una tÃ©cnica de optimizaciÃ³n para mejorar su desempeÃ±o."],"metadata":{"id":"bqRienFDPYiV"}},{"cell_type":"markdown","source":["### Importar librerÃ­as"],"metadata":{"id":"t3bT1oFwPrRg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXIX09uMPLwg"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["### Generar datos sintÃ©ticos con ruido"],"metadata":{"id":"9w0XVGy9PxNc"}},{"cell_type":"code","source":["np.random.seed(42)\n","# Fija la semilla del generador de nÃºmeros aleatorios para que los resultados sean reproducibles.\n","# Es Ãºtil para garantizar que obtengas los mismos resultados cada vez que ejecutes el cÃ³digo.\n","\n","x = np.linspace(0, 10, 150)\n","# Genera 150 puntos equiespaciados entre 0 y 10.\n","# Estos valores simulan las entradas (features) de un conjunto de datos.\n","\n","y = 2.5 * x + 1.0 + np.random.normal(0, 2, size=x.shape)\n","# Genera los valores de salida (targets) con una relaciÃ³n lineal: y = 2.5x + 1.0\n","# A esto se le suma ruido aleatorio con distribuciÃ³n normal (media = 0, desviaciÃ³n estÃ¡ndar = 2)\n","# para simular datos reales con variabilidad."],"metadata":{"id":"lQ7Iw4G8PzN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### FunciÃ³n de entrenamiento con gradiente descendente"],"metadata":{"id":"L1NuxPBFP5zf"}},{"cell_type":"code","source":["def entrenar_regresion_lineal(x, y, learning_rate, epochs):\n","    w = 0.0  # Inicializa el peso (pendiente de la recta) en cero\n","    b = 0.0  # Inicializa el sesgo/intercepto tambiÃ©n en cero\n","    n = len(x)  # NÃºmero de datos de entrada\n","    losses = []  # Lista para registrar la pÃ©rdida en cada iteraciÃ³n\n","\n","    for i in range(epochs):\n","        y_pred = w * x + b  # Calcula las predicciones con los parÃ¡metros actuales\n","        error = y_pred - y  # Calcula el error (diferencia entre predicciÃ³n y valor real)\n","\n","        # CÃ¡lculo del gradiente (derivadas parciales de la funciÃ³n de pÃ©rdida con respecto a w y b)\n","        dw = (2/n) * np.dot(error, x)  # Derivada respecto al peso w\n","        db = (2/n) * np.sum(error)     # Derivada respecto al sesgo b\n","\n","        # Actualiza los parÃ¡metros con gradiente descendente\n","        w -= learning_rate * dw  # Nuevo valor del peso\n","        b -= learning_rate * db  # Nuevo valor del sesgo\n","\n","        # Calcula y guarda el error cuadrÃ¡tico medio (MSE) en esta iteraciÃ³n\n","        loss = np.mean(error ** 2)\n","        losses.append(loss)\n","\n","        # Imprime el progreso cada 10 iteraciones o en la Ãºltima\n","        if i % 10 == 0 or i == epochs - 1:\n","            print(f\"IteraciÃ³n {i:3d}: w = {w:.4f}, b = {b:.4f}, Loss = {loss:.4f}\")\n","\n","    return w, b, losses"],"metadata":{"id":"wqvbRjrnP584"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Entrenar modelo y mostrar resultados"],"metadata":{"id":"-ouY4S3gQIAZ"}},{"cell_type":"code","source":["# Entrenar modelo\n","w_final, b_final, historial_loss = entrenar_regresion_lineal(x, y, learning_rate=0.01, epochs=100)\n","\n","# Mostrar resultados\n","print(f\"\\nModelo final: y = {w_final:.4f}x + {b_final:.4f}\")"],"metadata":{"id":"fz2sFOnmQIJY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Graficar resultados"],"metadata":{"id":"N86dP6BNQPtL"}},{"cell_type":"code","source":["plt.figure(figsize=(14, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.scatter(x, y, label=\"Datos reales\")\n","plt.plot(x, w_final * x + b_final, color='red', label=\"Modelo ajustado\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.title(\"RegresiÃ³n lineal con gradiente descendente\")\n","plt.legend()\n","plt.grid(True)\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(historial_loss)\n","plt.xlabel(\"IteraciÃ³n\")\n","plt.ylabel(\"PÃ©rdida (MSE)\")\n","plt.title(\"DisminuciÃ³n del error durante el entrenamiento\")\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"HsoHVYAtQP0x"},"execution_count":null,"outputs":[]}]}