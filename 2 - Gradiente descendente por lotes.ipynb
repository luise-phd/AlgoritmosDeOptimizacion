{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNBWdl+NO62quKr41dqlBUK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Problema\n","Ajustar una línea recta $𝑦=𝑤𝑥+𝑏$ a un conjunto de datos, minimizando el error cuadrático medio (MSE) entre las predicciones y los valores reales. En otras palabras, encontrar una función lineal que relacione una variable independiente 𝑥 con una variable dependiente 𝑦.\n","\n","Donde:\n","\n","* 𝑦 → es la variable objetivo (lo que queremos predecir).\n","* 𝑥 → es la variable de entrada o predictor.\n","* 𝑤 → es el peso o pendiente de la recta (cuánto cambia 𝑦 por cada unidad que cambia 𝑥).\n","* 𝑏 → es el sesgo o intersección con el eje Y (el valor de 𝑦 cuando $𝑥=0$).\n","\n","El objetivo es encontrar los valores óptimos de 𝑤 y 𝑏 que permitan que esta recta se aproxime lo mejor posible a los datos reales. Es decir, para cada punto de datos (𝑥𝑖, 𝑦𝑖), queremos que la predicción:\n","\n","$$\n","\\hat{y}_i = w x_i + b\n","$$\n","\n","sea lo más cercana posible a la verdadera salida 𝑦𝑖.\n","\n","Este ejemplo es una forma básica de inteligencia artificial porque implica que un modelo aprende automáticamente una relación entre variables a partir de datos, usando una técnica de optimización para mejorar su desempeño."],"metadata":{"id":"bqRienFDPYiV"}},{"cell_type":"markdown","source":["### Importar librerías"],"metadata":{"id":"t3bT1oFwPrRg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXIX09uMPLwg"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["### Generar datos sintéticos con ruido"],"metadata":{"id":"9w0XVGy9PxNc"}},{"cell_type":"code","source":["np.random.seed(42)\n","# Fija la semilla del generador de números aleatorios para que los resultados sean reproducibles.\n","# Es útil para garantizar que obtengas los mismos resultados cada vez que ejecutes el código.\n","\n","x = np.linspace(0, 10, 150)\n","# Genera 150 puntos equiespaciados entre 0 y 10.\n","# Estos valores simulan las entradas (features) de un conjunto de datos.\n","\n","y = 2.5 * x + 1.0 + np.random.normal(0, 2, size=x.shape)\n","# Genera los valores de salida (targets) con una relación lineal: y = 2.5x + 1.0\n","# A esto se le suma ruido aleatorio con distribución normal (media = 0, desviación estándar = 2)\n","# para simular datos reales con variabilidad."],"metadata":{"id":"lQ7Iw4G8PzN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Función de entrenamiento con gradiente descendente"],"metadata":{"id":"L1NuxPBFP5zf"}},{"cell_type":"code","source":["def entrenar_regresion_lineal(x, y, learning_rate, epochs):\n","    w = 0.0  # Inicializa el peso (pendiente de la recta) en cero\n","    b = 0.0  # Inicializa el sesgo/intercepto también en cero\n","    n = len(x)  # Número de datos de entrada\n","    losses = []  # Lista para registrar la pérdida en cada iteración\n","\n","    for i in range(epochs):\n","        y_pred = w * x + b  # Calcula las predicciones con los parámetros actuales\n","        error = y_pred - y  # Calcula el error (diferencia entre predicción y valor real)\n","\n","        # Cálculo del gradiente (derivadas parciales de la función de pérdida con respecto a w y b)\n","        dw = (2/n) * np.dot(error, x)  # Derivada respecto al peso w\n","        db = (2/n) * np.sum(error)     # Derivada respecto al sesgo b\n","\n","        # Actualiza los parámetros con gradiente descendente\n","        w -= learning_rate * dw  # Nuevo valor del peso\n","        b -= learning_rate * db  # Nuevo valor del sesgo\n","\n","        # Calcula y guarda el error cuadrático medio (MSE) en esta iteración\n","        loss = np.mean(error ** 2)\n","        losses.append(loss)\n","\n","        # Imprime el progreso cada 10 iteraciones o en la última\n","        if i % 10 == 0 or i == epochs - 1:\n","            print(f\"Iteración {i:3d}: w = {w:.4f}, b = {b:.4f}, Loss = {loss:.4f}\")\n","\n","    return w, b, losses"],"metadata":{"id":"wqvbRjrnP584"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Entrenar modelo y mostrar resultados"],"metadata":{"id":"-ouY4S3gQIAZ"}},{"cell_type":"code","source":["# Entrenar modelo\n","w_final, b_final, historial_loss = entrenar_regresion_lineal(x, y, learning_rate=0.01, epochs=100)\n","\n","# Mostrar resultados\n","print(f\"\\nModelo final: y = {w_final:.4f}x + {b_final:.4f}\")"],"metadata":{"id":"fz2sFOnmQIJY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Graficar resultados"],"metadata":{"id":"N86dP6BNQPtL"}},{"cell_type":"code","source":["plt.figure(figsize=(14, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.scatter(x, y, label=\"Datos reales\")\n","plt.plot(x, w_final * x + b_final, color='red', label=\"Modelo ajustado\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.title(\"Regresión lineal con gradiente descendente\")\n","plt.legend()\n","plt.grid(True)\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(historial_loss)\n","plt.xlabel(\"Iteración\")\n","plt.ylabel(\"Pérdida (MSE)\")\n","plt.title(\"Disminución del error durante el entrenamiento\")\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"HsoHVYAtQP0x"},"execution_count":null,"outputs":[]}]}