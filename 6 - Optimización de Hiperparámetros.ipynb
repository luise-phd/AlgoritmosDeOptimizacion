{"cells":[{"cell_type":"markdown","id":"e655295b","metadata":{"id":"e655295b"},"source":["## Optimización de Hiperparámetros\n","https://scikit-learn.org/stable/modules/grid_search.html\n","<p style='text-align: justify'>El autoajuste u optimización de hiperparámetros, también conocido como búsqueda de hiperparámetros, se refiere al proceso de encontrar la mejor combinación de configuraciones para un modelo de aprendizaje automático. Los hiperparámetros son configuraciones ajustables que no se aprenden directamente del conjunto de datos, como la profundidad de un árbol de decisión, la tasa de aprendizaje en una red neuronal o la regularización en un modelo de regresión.</p>\n","\n","<p style='text-align: justify'>El objetivo del autoajuste de hiperparámetros es optimizar estas configuraciones para mejorar el rendimiento del modelo en términos de una métrica específica, como Accuracy, F1-Score, R-cuadrado, entre otros. Este proceso implica probar diferentes combinaciones de hiperparámetros y evaluar el rendimiento del modelo utilizando validación cruzada u otros métodos de evaluación para determinar cuáles configuraciones proporcionan el mejor rendimiento.</p>\n","\n","<p style='text-align: justify'>Se utilizan técnicas como la búsqueda aleatoria, búsqueda en cuadrícula (grid search), optimización bayesiana, entre otros, para explorar el espacio de hiperparámetros y encontrar la combinación óptima que maximice el rendimiento del modelo en el conjunto de datos. El autoajuste de hiperparámetros es crucial para mejorar la capacidad de generalización y el rendimiento predictivo del modelo.</p>"]},{"cell_type":"markdown","id":"fc573f81","metadata":{"id":"fc573f81"},"source":["#### Otras ténicas para la Optimización de Hiperparámetros\n","1. <b>Optimización Bayesiana</b>: Utiliza métodos bayesianos para encontrar la combinación óptima de hiperparámetros al seleccionar de manera iterativa las configuraciones más prometedoras para evaluar. Métodos como Gaussian Process-based Optimization (GPO) o Tree-structured Parzen Estimator (TPE) son populares en esta área.\n","\n","2. <b>Optimización Evolutiva</b>: Se inspira en conceptos evolutivos y utiliza algoritmos genéticos, estrategias de evolución diferencial u otros métodos similares para buscar eficientemente en el espacio de hiperparámetros en busca de la mejor configuración.\n","\n","3. <b>Optimización basada en gradiente</b>: En lugar de explorar aleatoriamente o mediante una búsqueda sistemática, utiliza la información del gradiente de una métrica de evaluación para ajustar los hiperparámetros de manera iterativa, siguiendo una dirección que maximice o minimice esta métrica.\n","\n","4. <b>Optimización de Hyperband</b>: Esta técnica combina la exploración aleatoria con la eliminación temprana de configuraciones ineficaces. Se ejecutan múltiples configuraciones de forma aleatoria, pero se descartan rápidamente aquellas que no ofrecen buenos resultados, lo que permite enfocarse en las configuraciones prometedoras.\n","\n","Cada una de estas técnicas tiene sus propias ventajas y desventajas, y la elección de la técnica de búsqueda de hiperparámetros depende del conjunto de datos, el tiempo de cómputo disponible y las características del problema que se esté abordando."]},{"cell_type":"markdown","id":"6093cde7","metadata":{"id":"6093cde7"},"source":["### Importar librerías"]},{"cell_type":"code","execution_count":null,"id":"13eb3bbb","metadata":{"id":"13eb3bbb"},"outputs":[],"source":["import pandas as pd\n","\n","# Importar la función para cargar los conjuntos de datos desde sklearn\n","from sklearn.datasets import load_iris, load_diabetes\n","\n","# Importar funciones para dividir datos y realizar búsquedas de hiperparámetros\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n","\n","# Importar el clasificador RandomForestClassifier de la biblioteca sklearn.ensemble\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Importar el algoritmo  de regresión Support Vector Regressor\n","from sklearn.svm import SVR\n","\n","# Importar la distribución randint para definir rangos de valores en la búsqueda aleatoria\n","from scipy.stats import randint, uniform\n","\n","# Ignorar las advertencias\n","# import warnings\n","# warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"1b49bb82","metadata":{"id":"1b49bb82"},"source":["### Modelo de clasificación Random Forest"]},{"cell_type":"markdown","source":["<p style='text-align: justify'>Random Forest (Bosque Aleatorio) es un algoritmo de aprendizaje automático que se utiliza para tareas de clasificación y regresión. Se basa en la construcción de múltiples árboles de decisión y combina sus resultados para mejorar la precisión y la robustez de las predicciones.</p>\n","\n","<p style='text-align: justify'>La idea central detrás del Random Forest es crear un conjunto de árboles de decisión en lugar de depender de un solo árbol. Cada árbol se construye utilizando una muestra aleatoria de los datos de entrenamiento y utilizando una técnica llamada \"bagging\". En este proceso:</p>\n","\n","* Se toma una muestra aleatoria (con reemplazo) de los datos de entrenamiento.\n","* Se construye un árbol de decisión utilizando la muestra seleccionada.\n","* Se repiten los pasos 1 y 2 para construir múltiples árboles.\n","\n","<p style='text-align: justify'>Luego, cuando se hace una predicción, cada árbol en el conjunto emite su propia predicción y la predicción final se determina por votación (en el caso de clasificación) o promedio (en el caso de regresión) de las predicciones de todos los árboles.</p>\n","\n","Las ventajas de Random Forest incluyen:\n","\n","* <b>Reducción del sobreajuste (overfitting)</b>: El ensamblaje de múltiples árboles reduce el riesgo de sobreajuste en comparación con un solo árbol.\n","* <b>Mayor estabilidad</b>: Los errores en la predicción de un solo árbol pueden ser compensados por otros árboles en el conjunto.\n","* Capacidad para manejar conjuntos de datos grandes y dimensiones altas.\n","* Buena capacidad de manejar características categóricas y numéricas sin mucho preprocesamiento.\n","\n","<p style='text-align: justify'>Random Forest se utiliza ampliamente en aplicaciones del mundo real, como clasificación de imágenes, detección de fraudes, análisis de mercado y más. Es una técnica versátil y efectiva para mejorar la precisión y la generalización de los modelos de aprendizaje automático.</p>"],"metadata":{"id":"jXI_EBEnbSFE"},"id":"jXI_EBEnbSFE"},{"cell_type":"markdown","id":"aa3bcd87","metadata":{"id":"aa3bcd87"},"source":["<p style='text-align: justify'>En este ejemplo se muestra  cómo realizar la búsqueda de hiperparámetros utilizando la búsqueda aleatoria (RandomizedSearchCV) y la búsqueda en cuadrícula (GridSearchCV) con un clasificador de Bosques Aleatorios (RandomForestClassifier) en Scikit-learn.</p>"]},{"cell_type":"code","execution_count":null,"id":"85f2c583","metadata":{"id":"85f2c583"},"outputs":[],"source":["# Cargamos un conjunto de datos de ejemplo (Iris)\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Dividimos los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","id":"a50fc02f","metadata":{"id":"a50fc02f"},"source":["#### Conjunto de datos Iris\n","<p style='text-align: justify'>El conjunto de datos Iris es un conjunto clásico y comúnmente utilizado en la comunidad de Machine Learning. Fue introducido por el biólogo y estadístico británico Ronald Fisher en 1936. Este conjunto de datos contiene información sobre tres especies de iris: Setosa, Versicolor y Virginica.</p>\n","\n","<p style='text-align: justify'>El conjunto de datos Iris consta de 150 muestras de iris, donde cada especie de iris se representa con 50 muestras. Cada muestra tiene cuatro características: longitud y ancho del sépalo, y longitud y ancho del pétalo, todas en centímetros. Estas características se utilizan para predecir la especie de iris.</p>\n","\n","<p style='text-align: justify'>El conjunto de datos Iris es un excelente punto de partida para tareas de clasificación y aprendizaje automático, ya que es pequeño, fácil de entender y tiene características numéricas bien definidas que permiten aplicar algoritmos de clasificación para predecir la especie de flor de iris basándose en sus características morfológicas.</p>"]},{"cell_type":"code","execution_count":null,"id":"e09b1ef1","metadata":{"id":"e09b1ef1"},"outputs":[],"source":["print(iris.DESCR)"]},{"cell_type":"markdown","source":["### Convertir a DataFrame para visualizar el conjunto de datos"],"metadata":{"id":"I4zUKklgZm10"},"id":"I4zUKklgZm10"},{"cell_type":"code","source":["df = pd.DataFrame(iris.data, columns=iris.feature_names)\n","\n","# Agregar la columna de etiquetas\n","df['target'] = iris.target\n","\n","# Mostrar las primeras 5 filas\n","df.head()"],"metadata":{"id":"XvEYowmyZoHG"},"id":"XvEYowmyZoHG","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dimensiones del dataset"],"metadata":{"id":"KpefLoEnaLc2"},"id":"KpefLoEnaLc2"},{"cell_type":"code","source":["print(df.shape)\n","print('Cantidad de registros:', df.shape[0])\n","print('Cantidad de variables:', df.shape[1])"],"metadata":{"id":"QTeiuAFUaMhB"},"id":"QTeiuAFUaMhB","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"bbd16f85","metadata":{"id":"bbd16f85"},"source":["### Búsqueda aleatoria\n","<p style='text-align: justify'>La Búsqueda Aleatoria, o Randomized Search, es una técnica de optimización de hiperparámetros en la que se exploran diferentes combinaciones de hiperparámetros seleccionadas aleatoriamente dentro de un rango definido. En contraste con la Búsqueda en Cuadrícula (Grid Search), que evalúa todas las combinaciones posibles dentro de un conjunto predefinido de valores, la Búsqueda Aleatoria selecciona muestras aleatorias de un espacio de búsqueda.</p>\n","\n","<p style='text-align: justify'>Esta estrategia es útil cuando el espacio de búsqueda de hiperparámetros es grande y no es factible evaluar todas las combinaciones posibles. En lugar de examinar exhaustivamente todas las combinaciones, la Búsqueda Aleatoria selecciona de manera aleatoria un número determinado de configuraciones de hiperparámetros para evaluar. Esta técnica puede resultar más eficiente en tiempo y recursos, especialmente en conjuntos de datos grandes o con modelos complejos.</p>\n","\n","<p style='text-align: justify'>Al probar combinaciones aleatorias de hiperparámetros, la Búsqueda Aleatoria permite explorar el espacio de búsqueda de manera más rápida y, en muchos casos, puede encontrar combinaciones que conduzcan a buenos resultados sin necesidad de probar todas las opciones.</p>"]},{"cell_type":"code","execution_count":null,"id":"fc4bf3a6","metadata":{"id":"fc4bf3a6"},"outputs":[],"source":["%%time\n","\n","# Definimos los hiperparámetros para la búsqueda aleatoria\n","# Se utiliza la librería scipy.stats y el módulo randint para\n","# generar números enteros aleatorios dentro de un rango especificado.\n","param_dist = {\n","    'n_estimators': randint(100, 150), # Número de árboles en el bosque aleatorio\n","    'max_depth': randint(3, 10), # Profundidad máxima de cada árbol en el bosque\n","    'min_samples_split': randint(2, 10), # Número mínimo de muestras requeridas para dividir un nodo interno\n","    'min_samples_leaf': randint(1, 5), # Número mínimo de muestras necesarias para que un nodo sea considerado hoja\n","    'bootstrap': [True, False] # Parámetro booleano que indica si se debe usar o no, el muestreo con reemplazo\n","}\n","\n","# Creamos un clasificador de Bosques Aleatorios\n","rf = RandomForestClassifier()\n","\n","# Búsqueda aleatoria\n","# cv=5: Indica la estrategia de validación cruzada a utilizar. Aquí, se emplea una validación cruzada estratificada\n","# de 5-folds para evaluar el rendimiento del modelo en diferentes subconjuntos del conjunto de datos.\n","random_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=15, cv=5, random_state=42)\n","random_search.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"0cc1bc11","metadata":{"id":"0cc1bc11"},"source":["#### Hiperparámetros:\n","* <b>n_estimators</b>: randint(100, 150): Este parámetro 'n_estimators' determina el número de árboles en el bosque aleatorio. Aquí, randint(100, 150) genera un número entero aleatorio en el rango de 100 a 999 (100 a 150 excluyendo el límite superior), especificando así la cantidad posible de estimadores (árboles) que se pueden utilizar en el bosque aleatorio.\n","\n","* <b>max_depth</b>: randint(3, 10): El parámetro 'max_depth' representa la profundidad máxima de cada árbol en el bosque. randint(3, 10) genera un número entero aleatorio entre 3 y 9 (3 a 10 excluyendo el límite superior), definiendo el rango de profundidad máxima para los árboles.\n","\n","* <b>min_samples_split</b>: randint(2, 10): Este hiperparámetro indica el número mínimo de muestras requeridas para dividir un nodo interno. Con randint(2, 10) se genera un número entero aleatorio entre 2 y 9 (2 a 10 excluyendo el límite superior) que define el rango de posibles valores para la cantidad mínima de muestras requeridas para dividir un nodo.\n","\n","* <b>min_samples_leaf</b>: randint(1, 5): El hiperparámetro 'min_samples_leaf' establece el número mínimo de muestras necesarias para que un nodo sea considerado hoja. randint(1, 5) genera un número entero aleatorio entre 1 y 4 (1 a 5 excluyendo el límite superior) que especifica el rango de posibles valores para el número mínimo de muestras en una hoja.\n","\n","* <b>bootstrap</b>: [True, False]: 'bootstrap' es un parámetro booleano que indica si se debe usar o no el muestreo con reemplazo. Aquí, se crea una lista con dos opciones: True o False, lo que permitirá evaluar si se debe usar el bootstrap (muestreo con reemplazo) o no en el RandomForestClassifier."]},{"cell_type":"markdown","id":"04d55470","metadata":{"id":"04d55470"},"source":["### Evaluación del modelo"]},{"cell_type":"code","execution_count":null,"id":"5b496135","metadata":{"id":"5b496135"},"outputs":[],"source":["# Muestra los mejores hiperparámetros de la búsqueda aleatoria\n","print(\"Mejores hiperparámetros de Random Search:\\n{}\\n\".format(random_search.best_params_))\n","\n","# Evalua los modelos en el conjunto de prueba\n","random_search_score = random_search.score(X_test, y_test)\n","\n","print(\"Puntuación de Random Search en el conjunto de prueba:\", random_search_score)"]},{"cell_type":"markdown","id":"9acf9fb5","metadata":{"id":"9acf9fb5"},"source":["### Búsqueda en cuadrícula\n","<p style='text-align: justify'>La Búsqueda en Cuadrícula, o Grid Search, es una técnica de optimización de hiperparámetros que evalúa exhaustivamente un conjunto predefinido de combinaciones de hiperparámetros para determinar la configuración óptima que maximiza el rendimiento del modelo.</p>\n","\n","<p style='text-align: justify'>En esta estrategia, se define un conjunto de valores posibles para cada hiperparámetro que se desea ajustar y se genera un \"grid\" o cuadrícula con todas las posibles combinaciones de estos valores. Luego, se entrena y evalúa el modelo utilizando cada combinación de hiperparámetros dentro de esta cuadrícula, y se selecciona aquella configuración que ofrezca el mejor rendimiento de acuerdo con una métrica de evaluación específica, como precisión, F1-score, AUC, entre otras.</p>\n","\n","<p style='text-align: justify'>Aunque la Búsqueda en Cuadrícula es exhaustiva y garantiza evaluar todas las combinaciones posibles dentro del espacio definido, puede resultar costosa computacionalmente en comparación con otras técnicas, especialmente cuando el espacio de búsqueda es grande o cuando se tienen conjuntos de datos extensos. Sin embargo, proporciona una manera sistemática de encontrar la mejor combinación de hiperparámetros para un modelo.</p>"]},{"cell_type":"code","execution_count":null,"id":"e588acf0","metadata":{"id":"e588acf0"},"outputs":[],"source":["%%time\n","\n","# Definimos los hiperparámetros para la búsqueda en cuadrícula\n","param_grid = {\n","    'n_estimators': [100, 110, 120],\n","    'max_depth': [3, 5, 7],\n","    'min_samples_split': [2, 5],\n","    'min_samples_leaf': [1, 2],\n","    'bootstrap': [True, False]\n","}\n","\n","# Búsqueda en cuadrícula\n","grid_search = GridSearchCV(rf, param_grid=param_grid, cv=5)\n","grid_search.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"0983e491","metadata":{"id":"0983e491"},"source":["### Evaluación del modelo"]},{"cell_type":"code","execution_count":null,"id":"e250e617","metadata":{"id":"e250e617"},"outputs":[],"source":["# Obtenemos los mejores hiperparámetros de la búsqueda en cuadrícula\n","print(\"Mejores hiperparámetros de Grid Search:\\n{}\\n\".format(grid_search.best_params_))\n","\n","# Evaluamos los modelos en el conjunto de prueba\n","grid_search_score = grid_search.score(X_test, y_test)\n","\n","print(\"Puntuación de Grid Search en el conjunto de prueba:\", grid_search_score)"]},{"cell_type":"markdown","id":"5c4136b8","metadata":{"id":"5c4136b8"},"source":["<p style='text-align: justify'>En ambos tipos de búsqueda, ya sea con RandomizedSearchCV o GridSearchCV, la métrica predeterminada que se utiliza para evaluar la calidad del modelo en cada combinación de hiperparámetros es la precisión (accuracy) en el caso de problemas de clasificación.</p>\n","\n","<p style='text-align: justify'>La precisión es una métrica comúnmente utilizada en problemas de clasificación y representa la proporción de predicciones correctas realizadas por el modelo sobre el total de predicciones realizadas. Para cada combinación de hiperparámetros evaluada durante la búsqueda, se calcula la precisión utilizando validación cruzada (o alguna otra estrategia de validación) y se utiliza para comparar y seleccionar los mejores hiperparámetros para el modelo.</p>"]},{"cell_type":"markdown","id":"f1d4ac45","metadata":{"id":"f1d4ac45"},"source":["#### Conclusión\n","<p style='text-align: justify'>El conjunto de datos Iris es conocido por ser un conjunto de datos relativamente pequeño y limpio, con clases bien separadas, lo que puede resultar en una alta precisión para varios algoritmos de clasificación. En el ejemplo se obtuvo una precisión de 1.0 (o 100%) durante la búsqueda de hiperparámetros, eso es justificable debido a la naturaleza de este conjunto de datos.</p>\n","\n","<p style='text-align: justify'>El conjunto de datos Iris consta de tres clases de plantas que son distinguibles mediante características específicas como longitud y ancho del sépalo y pétalo. Dado que estas clases son altamente separables y los modelos de clasificación pueden aprender fácilmente a distinguirlas, es posible lograr una alta precisión, especialmente con algoritmos como Random Forest, que tienden a funcionar bien en conjuntos de datos pequeños y limpios.</p>\n","\n","<p style='text-align: justify'>Por lo tanto, en el contexto del conjunto de datos Iris, es posible obtener resultados de precisión altos sin que necesariamente indiquen sobreajuste o errores en el modelo.</p>"]},{"cell_type":"markdown","id":"56b63bd3","metadata":{"id":"56b63bd3"},"source":["### Modelo de regresión Support Vector Regressor (SVR)\n","\n","<p style='text-align: justify'>SVR (Support Vector Regression) es una técnica de regresión utilizada en aprendizaje automático. Se basa en el concepto de Máquinas de soporte vectorial (SVM) que se emplea en problemas de clasificación y se extiende para resolver problemas de regresión.</p>\n","\n","<p style='text-align: justify'>La SVR busca encontrar una función de regresión en un espacio de alta dimensión, donde los puntos de datos, representados como vectores, están separados por un margen con el objetivo de minimizar el error de predicción. Funciona encontrando el hiperplano que maximiza el margen alrededor de los puntos de datos más cercanos, denominados vectores de soporte. A diferencia de la regresión lineal, SVR puede manejar relaciones no lineales mediante el uso de funciones de kernel, permitiendo la transformación de los datos a un espacio de mayor dimensión donde los datos puedan ser linealmente separables.</p>\n","\n","<p style='text-align: justify'>El objetivo principal de SVR es minimizar la cantidad de errores, permitiendo cierto grado de error tolerable (εε). SVR intenta ajustar una función que se ajuste a la mayoría de los puntos de datos dentro del margen tolerable, al mismo tiempo que minimiza las desviaciones de los puntos de datos más cercanos. Esto lo logra mediante la optimización de una función de costo que equilibra la precisión del modelo y la amplitud del margen, controlada por parámetros como C (parámetro de regularización) y la elección del kernel.</p>"]},{"cell_type":"code","execution_count":null,"id":"ed0fe8f7","metadata":{"id":"ed0fe8f7"},"outputs":[],"source":["# Cargar el conjunto de datos\n","diabetes = load_diabetes()\n","X, y = diabetes.data, diabetes.target\n","\n","# Dividir el conjunto de datos en entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","id":"9d445b04","metadata":{"id":"9d445b04"},"source":["#### Conjunto de datos diabetes\n","<p style='text-align: justify'>El conjunto de datos diabetes de Scikit-learn es un conjunto de datos clásico que contiene información médica relevante para el estudio de la diabetes. Este conjunto de datos incluye diez variables fisiológicas (edad, sexo, índice de masa corporal, presión arterial y seis mediciones de suero sanguíneo) y una medida cuantitativa de la progresión de la enfermedad un año después del inicio del estudio.</p>\n","\n","<p style='text-align: justify'>Las variables son todos valores continuos y representan características médicas relevantes, mientras que el objetivo es una medida cuantitativa de la progresión de la enfermedad un año después del inicio del estudio.</p>\n","\n","<p style='text-align: justify'>El conjunto de datos diabetes se utiliza comúnmente para tareas de regresión donde el objetivo es predecir la progresión de la enfermedad basándose en las características médicas proporcionadas.</p>"]},{"cell_type":"code","execution_count":null,"id":"4334142e","metadata":{"id":"4334142e"},"outputs":[],"source":["print(diabetes.DESCR)"]},{"cell_type":"markdown","id":"d56d6090","metadata":{"id":"d56d6090"},"source":["### Búsqueda aleatoria"]},{"cell_type":"code","execution_count":null,"id":"33090377","metadata":{"id":"33090377"},"outputs":[],"source":["%%time\n","\n","# Definir los hiperparámetros para la búsqueda aleatoria\n","param_dist = {\n","    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Diferentes tipos de kernel\n","    'C': uniform(loc=0, scale=100),  # Parámetro de regularización (distribución uniforme de 0 a 10)\n","    'gamma': ['scale', 'auto'],  # Valores para gamma\n","    'epsilon': uniform(loc=0, scale=1)  # Parámetro de margen epsilon (distribución uniforme de 0 a 1)\n","}\n","\n","# Crear el modelo de regresión\n","svr = SVR()\n","\n","# Búsqueda aleatoria\n","random_search = RandomizedSearchCV(svr, param_distributions=param_dist, n_iter=100, cv=5, random_state=42)\n","random_search.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"4d58ea1e","metadata":{"id":"4d58ea1e"},"source":["#### Hiperparámetros:\n","* <b>kernel</b>: Lista que contiene diferentes tipos de kernel para SVR (lineal, polinómico, gaussiano RBF y sigmoidal).\n","* <b>C</b>: Distribución uniforme que genera valores de regularización C para SVR de forma aleatoria en el rango de 0 a 10. Este parámetro controla la penalización de errores.\n","* <b>gamma</b>: Lista que incluye opciones para el parámetro gamma en SVR, con los valores 'scale' y 'auto'. 'scale' usa por defecto 1 / (n_features * X.var()) como valor de gamma, mientras que 'auto' usa 1 / n_features.\n","* <b>epsilon</b>: Distribución uniforme que genera valores para el margen epsilon de SVR en el rango de 0 a 1. Este parámetro controla la sensibilidad de la función de pérdida."]},{"cell_type":"markdown","id":"d27a6466","metadata":{"id":"d27a6466"},"source":["### Evaluación del modelo"]},{"cell_type":"code","execution_count":null,"id":"88579a47","metadata":{"id":"88579a47"},"outputs":[],"source":["# Mejores hiperparámetros encontrados mediante búsqueda aleatoria\n","print(\"\\nMejores hiperparámetros encontrados mediante búsqueda aleatoria:\")\n","print(random_search.best_params_)\n","\n","# R cuadrado en entrenamiento y prueba con los mejores hiperparámetros\n","print(\"\\nR² en entrenamiento:\", random_search.score(X_train, y_train))\n","print(\"R² en prueba:\", random_search.score(X_test, y_test))"]},{"cell_type":"markdown","id":"39d5c239","metadata":{"id":"39d5c239"},"source":["### Búsqueda en cuadrícula"]},{"cell_type":"code","execution_count":null,"id":"cf335cff","metadata":{"id":"cf335cff"},"outputs":[],"source":["%%time\n","\n","# Búsqueda en cuadrícula alrededor de los mejores hiperparámetros encontrados\n","param_grid = {\n","    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Diferentes tipos de kernel\n","    'C': [0.1, 1, 10, 100],\n","    'gamma': ['scale', 'auto'],  # Valores para gamma\n","    'epsilon': list(uniform(loc=0, scale=1).rvs(10))\n","}\n","\n","# Búsqueda en cuadrícula\n","grid_search = GridSearchCV(svr, param_grid=param_grid, cv=5)\n","grid_search.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"6f6ddcd7","metadata":{"id":"6f6ddcd7"},"source":["### Evaluación del modelo"]},{"cell_type":"code","execution_count":null,"id":"cca07d4e","metadata":{"id":"cca07d4e"},"outputs":[],"source":["# Mejores hiperparámetros encontrados mediante búsqueda en cuadrícula\n","print(\"\\nMejores hiperparámetros encontrados mediante búsqueda en cuadrícula:\")\n","print(grid_search.best_params_)\n","\n","# R cuadrado en entrenamiento y prueba con los mejores hiperparámetros\n","print(\"\\nR² en entrenamiento:\", grid_search.score(X_train, y_train))\n","print(\"R² en prueba:\", grid_search.score(X_test, y_test))"]},{"cell_type":"markdown","id":"980212f6","metadata":{"id":"980212f6"},"source":["#### Conclusión\n","<p style='text-align: justify'>El coeficiente de determinación (R-cuadrado) es una medida que indica la proporción de la varianza en la variable dependiente que es predecible a partir de las variables independientes en un modelo de regresión. Un valor de R-cuadrado cercano a 1 indica un buen ajuste del modelo a los datos, donde el modelo explica una gran parte de la variabilidad de la variable dependiente. Un valor cercano a 0.5 sugiere que el modelo está capturando una parte significativa, pero no toda, de la variabilidad de los datos.</p>\n","\n","<p style='text-align: justify'>En el caso específico del SVR (Support Vector Regression), el R-cuadrado alrededor de 0.5 puede significar que el modelo está capturando parte de la variabilidad en el conjunto de datos diabetes de Scikit-learn. Sin embargo, es importante tener en cuenta que diferentes conjuntos de datos pueden mostrar diferentes niveles de predictibilidad debido a su naturaleza.</p>\n","\n","<p style='text-align: justify'>Un R-cuadrado de 0.5 también podría indicar que el modelo no puede explicar completamente la variabilidad de la variable dependiente con las características proporcionadas, lo que puede ser común en conjuntos de datos complejos o en problemas donde hay múltiples factores influyentes que no están siendo considerados por el modelo. En algunos casos, un R-cuadrado de 0.5 puede considerarse aceptable dependiendo del contexto del problema y las expectativas del rendimiento del modelo.</p>"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}